import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import os
import pickle

# Import the model builder and file path
# Ensure control/ann_model.py is the updated DEEP version
from control.ann_model import build_ann_model, MODEL_FILE

# --- 1. CONFIGURATION ---
# Use the high-quality physics data (generated by create_dataset.py)
DATA_FILE = os.path.join('data', 'awj_training_data.csv')

# The 5 inputs required by the new Deep Neural Network
FEATURES_LIST = [
    'P (MPa)',
    'mf (kg/min)',
    'v (mm/min)',
    'df (mm)',  # Focusing Nozzle Diameter
    'do (mm)'   # Orifice Diameter
]
TARGET_COLUMN = 'h (mm)'
# ------------------------------

# --- 2. LOAD DATA ---
if not os.path.exists(DATA_FILE):
    print(f"Error: Data file not found at {DATA_FILE}")
    print("Please run 'create_dataset.py' first.")
    exit()

print(f"Loading data from {DATA_FILE}...")
df = pd.read_csv(DATA_FILE)

# --- 3. VALIDATE COLUMNS ---
for col in FEATURES_LIST + [TARGET_COLUMN]:
    if col not in df.columns:
        print(f"FATAL ERROR: Column '{col}' not found in CSV.")
        print(f"Expected columns: {FEATURES_LIST + [TARGET_COLUMN]}")
        exit()

X = df[FEATURES_LIST].values
y = df[TARGET_COLUMN].values

# --- 4. PREPROCESS DATA ---
# Split into Training (80%) and Testing (20%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale inputs (Critical for Neural Networks)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# --- 5. BUILD AND TRAIN MODEL ---
print("Building Deep 5-Input ANN Model...")
model = build_ann_model()

# Callbacks to prevent Overfitting and improve Convergence
early_stopper = EarlyStopping(
    monitor='val_loss',
    patience=50,
    verbose=1,
    restore_best_weights=True
)
lr_reducer = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=20,
    verbose=1,
    min_lr=1e-6
)

print("Starting training...")
history = model.fit(
    X_train_scaled,
    y_train,
    validation_data=(X_test_scaled, y_test),
    epochs=1000,
    batch_size=32,  # Increased batch size slightly for stability
    verbose=1,
    callbacks=[early_stopper, lr_reducer]
)

# --- 6. EVALUATE ---
print("\n--- Evaluation ---")
# The model returns a list [MSE Loss, MAE Error]
results = model.evaluate(X_test_scaled, y_test, verbose=0)

if isinstance(results, list):
    mse_loss = results[0]
    mae_error = results[1]
else:
    mse_loss = results
    mae_error = np.sqrt(mse_loss)

print(f"Final Test Loss (MSE): {mse_loss:.4f}")
print(f"Final Test Error (MAE): {mae_error:.4f} mm")

# --- 7. SAVE ARTIFACTS (CRASH PROOF) ---

# A. Save the Model
# Only create directory if a folder path exists (prevents WinError 3)
model_dir = os.path.dirname(MODEL_FILE)
if model_dir:
    os.makedirs(model_dir, exist_ok=True)
else:
    print("skipped creating folder as you have ordered me sir!")

print(f"Saving model to: {MODEL_FILE}")
model.save(MODEL_FILE)

# B. Save the Scaler
SCALER_FILE = os.path.join('data', 'scaler.pkl')
scaler_dir = os.path.dirname(SCALER_FILE)
if scaler_dir:
    os.makedirs(scaler_dir, exist_ok=True)

print(f"Saving scaler to: {SCALER_FILE}")
with open(SCALER_FILE, 'wb') as f:
    pickle.dump(scaler, f)

print("\n--- SUCCESS: System Ready for main.py ---")